{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template notebook for fitting synthetic (model-generated) data\n",
    "\n",
    "The synthetic data used for fitting needs to be contained in a netCDF file, just like the regular size-distribution data. One simple way to create a synthetic file is to follow these steps:\n",
    " 1. Run the `fit_models.ipynb` notebook setting the `prior_only = True`.\n",
    " 2. Select a sample from the output file (specified by the `savename_output` option).\n",
    " 3. Use the [extract_synthetic_data.py](../scripts/extract_synthetic_data.py) script to convert into a synthetic data file in the correct format.\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import netCDF4 as nc4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import pystan\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test data (not all data is used for fitting/training)\n",
    "use_testdata = False\n",
    "\n",
    "# create plots of the data\n",
    "show_data = True\n",
    "\n",
    "# add noise to synthetic data\n",
    "add_noise=True\n",
    "\n",
    "# the sigma parameter determining the amount of noise (higher value -> less noise)\n",
    "sigma_noise = 10000.\n",
    "\n",
    "# netCDF output file (set to None to not save output)\n",
    "savename_output = '../results/m_ftf_synthetic01.nc'\n",
    "\n",
    "# save the Stan output instead a few stats (only active if filename is specified above)\n",
    "save_stan_output = True\n",
    "\n",
    "save_only_converged = True\n",
    "\n",
    "# specify the Stan variable names to save; if set to None, all variables are saved \n",
    "# (only active if save_stan_output is True)\n",
    "varnames_save = None\n",
    "\n",
    "# the number of tries to fit each Stan model to achieve an R-hat < 1.1\n",
    "num_tries = 3\n",
    "\n",
    "# the number of chains to run\n",
    "num_chains = 4\n",
    "\n",
    "# the prior_only option passed to each Stan model\n",
    "prior_only = False\n",
    "\n",
    "# Number of days of data to fit\n",
    "limit_days = 2\n",
    "\n",
    "# Whether or not data limit is inclusive (include boundary point)\n",
    "inclusive = False\n",
    "\n",
    "# Whether to append dataset to itself to create a 96-hour dataset\n",
    "# This option is used for validation experiments in rolling_window.ipynb\n",
    "extend = False\n",
    "\n",
    "# Whether to append results to an existing file or overwrite\n",
    "append = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, plot synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file containing the synthetic data without added noise\n",
    "datafile = '../data/size_distribution/synthetic_m_ftf_01.nc'\n",
    "\n",
    "dataname = 'zinser'\n",
    "\n",
    "desc = 'synthetic dataset'\n",
    "\n",
    "# Indices of data to hold out for hold-out validation\n",
    "itestfile = None\n",
    "\n",
    "size_units = 'fg C cell$^{-1}$'\n",
    "\n",
    "def get_data(datafile, size_units, itestfile, dataname, desc, extend=False):\n",
    "    \n",
    "    data_gridded = {}\n",
    "    with nc4.Dataset(datafile) as nc:\n",
    "        for var in nc.variables:\n",
    "            data_gridded[var] = nc.variables[var][:]\n",
    "            \n",
    "    # create \"counts\" entry\n",
    "    if 'count' in data_gridded:\n",
    "        data_gridded['counts'] = (data_gridded['count'][None,:]\n",
    "                                  * data_gridded['w_obs']).astype(int)\n",
    "    else:\n",
    "        raise RuntimeError('Cannot find a \"count\" entry in \"{}\".'.format(datafile))\n",
    "    \n",
    "    \n",
    "    # Appends the time series to itself to create a pseudo four-day dataset\n",
    "    if extend:\n",
    "        data_gridded['time'] = np.concatenate((data_gridded['time'],\n",
    "                                               (data_gridded['time']\n",
    "                                               + data_gridded['time'][-1]\n",
    "                                               + data_gridded['time'][1])[:-1]))\n",
    "        \n",
    "        for item in ('w_obs', 'PAR', 'abundance', 'count', 'counts'):\n",
    "            if len(data_gridded[item].shape) == 2:\n",
    "                data_gridded[item] = np.concatenate((data_gridded[item],\n",
    "                                                     data_gridded[item][:, 1:]), axis=1)\n",
    "            else:\n",
    "                data_gridded[item] = np.concatenate((data_gridded[item],\n",
    "                                                     data_gridded[item][1:]))\n",
    "\n",
    "\n",
    "    # add description\n",
    "    desc += ' (m={data[m]}, $\\Delta_v^{{-1}}$={data[delta_v_inv]})'.format(data=data_gridded)\n",
    "    \n",
    "    return data_gridded, desc\n",
    "\n",
    "data_gridded, desc = get_data(datafile, size_units, itestfile, dataname, desc, extend=extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_colorbar(ax, **cbarargs):\n",
    "    axins_cbar = inset_axes(ax, width='3%', height='90%', loc=5,\n",
    "                            bbox_to_anchor=(0.05,0.0,1,1),\n",
    "                            bbox_transform=ax.transAxes)\n",
    "    mpl.colorbar.ColorbarBase(axins_cbar, orientation='vertical',\n",
    "                              **cbarargs)\n",
    "\n",
    "if show_data:\n",
    "    nrows = 3\n",
    "\n",
    "    v_min = data_gridded['v_min']\n",
    "    delta_v = 1.0/data_gridded['delta_v_inv']\n",
    "    v = v_min * 2**(np.arange(data_gridded['m'])*delta_v) \n",
    "\n",
    "    fig,axs = plt.subplots(nrows=nrows, sharex=True, figsize=(12,4*nrows))\n",
    "\n",
    "    ax = axs[0]\n",
    "    ax.set_title('raw '+desc, size=20)\n",
    "    ax.plot(data_gridded['time'], data_gridded['PAR'], color='gold')\n",
    "    ax.set(ylabel='PAR')\n",
    "\n",
    "    ax = axs[1]\n",
    "    pc = ax.pcolormesh(data_gridded['time'], v, data_gridded['w_obs'],\n",
    "                       shading='auto')\n",
    "    ax.set(ylabel='size ({})'.format(size_units))\n",
    "    add_colorbar(ax, norm=pc.norm, cmap=pc.cmap, label='size class proportion')\n",
    "\n",
    "    ax = axs[2]\n",
    "    pc = ax.pcolormesh(data_gridded['time'], v, data_gridded['counts'],\n",
    "                       shading='auto')\n",
    "    ax.set(ylabel='size ({})'.format(size_units))\n",
    "    add_colorbar(ax, norm=pc.norm, cmap=pc.cmap, label='counts')\n",
    "axs[-1].set_xlabel=('time (minutes)')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data, add noise, and re-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for Stan model\n",
    "def data_prep(data_gridded, dt=20, limit_days=2, start=0, use_testdata=False,\n",
    "              itestfile=None, prior_only=False, inclusive=False, add_noise=True, \n",
    "              sigma_noise=10000.0):\n",
    "    \n",
    "    data = {'dt':dt}\n",
    "    for v in ('m','v_min','delta_v_inv'):\n",
    "        data[v] = data_gridded[v]\n",
    "\n",
    "    data['obs'] = data_gridded['w_obs']\n",
    "    data['t_obs'] = data_gridded['time']\n",
    "    par = data_gridded['PAR']\n",
    "\n",
    "    # add noise\n",
    "    if add_noise:\n",
    "        for icol in range(data['obs'].shape[1]):\n",
    "            alpha = data['obs'][:, icol] / np.sum(data['obs'][:, icol]) * sigma_noise + 1.0\n",
    "            data['obs'][:, icol] = np.random.multinomial(data_gridded['count'][icol], \n",
    "                                                         np.random.dirichlet(alpha))\n",
    "        data['obs'] /= np.sum(data['obs'], axis=0)[None,:]\n",
    "    \n",
    "    if limit_days > 0:\n",
    "        limit_minutes = limit_days*1440\n",
    "\n",
    "        if inclusive:\n",
    "            ind_obs = (start*60 <= data['t_obs']) & (data['t_obs'] <= limit_minutes+start*60)\n",
    "        else:\n",
    "            ind_obs = (start*60 <= data['t_obs']) & (data['t_obs'] < limit_minutes+start*60)\n",
    "\n",
    "        if not np.all(ind_obs):\n",
    "            total = data['obs'].shape[1]\n",
    "            remove = total - data['obs'][:, ind_obs].shape[1]\n",
    "            print('start is set to {}, limit_days is set to {}, removing {}/{} observation times'.format(start,\n",
    "                                                                                                         limit_days,\n",
    "                                                                                                         remove,\n",
    "                                                                                                         total))\n",
    "\n",
    "        data['t_obs'] = data['t_obs'][ind_obs]\n",
    "        data['obs'] = data['obs'][:,ind_obs]\n",
    "\n",
    "        data['nt'] = int(limit_minutes//data['dt']+1)\n",
    "\n",
    "    data['nt_obs'] = data['t_obs'].size\n",
    "\n",
    "    if use_testdata:\n",
    "        # load cross-validation testing indices and add them to data\n",
    "        data['i_test'] = np.loadtxt(itestfile).astype(int)\n",
    "        # remove last index, so that dimensions agree\n",
    "        data['i_test'] = data['i_test'][:-1]\n",
    "    else:\n",
    "        # set all indices to zero\n",
    "        data['i_test'] = np.zeros(data['nt_obs'], dtype=int)\n",
    "\n",
    "    # switch on or off data fitting\n",
    "    data['prior_only'] = int(prior_only)\n",
    "\n",
    "    # add light data\n",
    "    t = np.arange(data['nt'])*data['dt'] + start*60\n",
    "    data['E'] = np.interp(t, xp=data_gridded['time'][ind_obs], fp=par[ind_obs])\n",
    "\n",
    "    # real count data\n",
    "    data['obs_count'] = data_gridded['counts'][:, ind_obs]\n",
    "    \n",
    "    data['start'] = start\n",
    "\n",
    "    # consistency check\n",
    "    if len(data['i_test']) != data['nt_obs']:\n",
    "        raise ValueError('Invalid number of testing indices (expected {}, got {}).'.format(data['nt_obs'],\n",
    "                                                                                       len(data['i_test'])))\n",
    "    return data\n",
    "\n",
    "\n",
    "data = data_prep(data_gridded, dt=20, limit_days=limit_days, start=0, use_testdata=use_testdata, \n",
    "                 itestfile=itestfile, prior_only=prior_only, inclusive=False, add_noise=add_noise,\n",
    "                 sigma_noise=sigma_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    nrows = 3\n",
    "\n",
    "    v_min = data['v_min']\n",
    "    delta_v = 1.0/data['delta_v_inv']\n",
    "    v = v_min * 2**(np.arange(data['m'])*delta_v) \n",
    "    t = np.arange(data['nt'])*data['dt']\n",
    "\n",
    "\n",
    "    fig,axs = plt.subplots(nrows=nrows, sharex=True, figsize=(12,4*nrows))\n",
    "\n",
    "    ax = axs[0]\n",
    "    ax.set_title('processed '+desc, size=20)\n",
    "    ax.plot(t, data['E'], color='gold')\n",
    "    ax.set(ylabel='E')\n",
    "\n",
    "    ax = axs[1]\n",
    "    pc = ax.pcolormesh(data['t_obs'], v, data['obs'], shading='auto')\n",
    "    ax.set(ylabel='size ({})'.format(size_units))\n",
    "    add_colorbar(ax, norm=pc.norm, cmap=pc.cmap,\n",
    "                 label='size class proportion')\n",
    "    ax.set_xlim(left=0.0)\n",
    "\n",
    "    ax = axs[2]\n",
    "    pc = ax.pcolormesh(data['t_obs'], v, data['obs_count'], shading='auto')\n",
    "    ax.set(ylabel='size ({})'.format(size_units))\n",
    "    add_colorbar(ax, norm=pc.norm, cmap=pc.cmap, label='counts')\n",
    "    ax.set_xlim(left=0.0)\n",
    "axs[-1].set_xlabel('time (minutes)')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model to fit\n",
    "\n",
    "Only run the model that generated the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code files\n",
    "stan_files = {\n",
    "    #'m_bmx': '../stan_code/m_bmx.stan',\n",
    "    #'m_bmb': '../stan_code/m_bmb.stan',\n",
    "    #'m_pmb': '../stan_code/m_pmb.stan',\n",
    "    #'m_fmb': '../stan_code/m_fmb.stan',\n",
    "    #'m_fmf': '../stan_code/m_fmf.stan',\n",
    "    #'m_btb': '../stan_code/m_btb.stan',\n",
    "    #'m_ptb': '../stan_code/m_ptb.stan',\n",
    "    #'m_ftb': '../stan_code/m_ftb.stan',\n",
    "    'm_ftf': '../stan_code/m_ftf.stan',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_rhat(fit):\n",
    "    s = fit.summary()\n",
    "    irhat = s['summary_colnames'].index(\"Rhat\")\n",
    "    return np.nanmax(s['summary'][:,irhat])\n",
    "\n",
    "if 'models' not in globals():\n",
    "    models = {}\n",
    "if 'num_tries' not in globals():\n",
    "    num_tries = 3\n",
    "    \n",
    "try_again = True\n",
    "refit_all = False\n",
    "\n",
    "refit_required = {}\n",
    "stan_base_code = {}\n",
    "for key, stan_file in stan_files.items():\n",
    "    with open(stan_file) as f:\n",
    "        stan_base_code[key] = f.read()\n",
    "\n",
    "stan_code = {}\n",
    "for model in stan_files.keys():\n",
    "    code_split = stan_base_code[model].split('\\n')\n",
    "    stan_code[model] = '\\n'.join(code_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model in stan_files.keys():\n",
    "    refit_required[model] = True\n",
    "    if model in models and models[model].model_code == stan_code[model]:\n",
    "        print('{}: unchanged code, not recompiling'.format(model))\n",
    "        refit_required[model] = False\n",
    "    else:\n",
    "        if model in models:\n",
    "            print('{}: code change detected, recompiling'.format(model))\n",
    "        else:\n",
    "            print('{}: compiling'.format(model))\n",
    "        models[model] = pystan.StanModel(model_code=stan_code[model],\n",
    "                                         model_name=model,\n",
    "                                         obfuscate_model_name=False)\n",
    "\n",
    "\n",
    "# run a bunch of experiments -- this may take a while\n",
    "for model in models:\n",
    "    for itry in range(num_tries):\n",
    "        t0 = time.time()\n",
    "        logging.disable(logging.WARNING)\n",
    "        mcmcs = models[model].sampling(data=data, iter=2000, chains=num_chains)\n",
    "        logging.disable(logging.NOTSET)\n",
    "        sampling_time = time.time() - t0  # in seconds\n",
    "        print('Model {} for {}-hour window starting at {} hours fit in {} minutes.'.format(model,\n",
    "                                                                                           limit_days*24+2*int(inclusive),\n",
    "                                                                                           data['start'],\n",
    "                                                                                           np.round(sampling_time/60, 2)))\n",
    "        # get max Rhat\n",
    "        rhat_max = get_max_rhat(mcmcs)\n",
    "        print('{}: in try {}/{} found Rhat={:.3f}'.format(model, itry+1, num_tries, rhat_max), end='')\n",
    "        if rhat_max < 1.1 or itry == num_tries - 1:\n",
    "            print()\n",
    "            break\n",
    "        print(', trying again')\n",
    "\n",
    "    print('{}'.format(model)) \n",
    "    print('\\n'.join(x for x in mcmcs.__str__().split('\\n') if '[' not in x))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if 'varnames_save' not in globals():\n",
    "        varnames_save = None\n",
    "\n",
    "    if savename_output is not None:\n",
    "        if model == tuple(models.keys())[0] and append == False:\n",
    "            mode = 'w'\n",
    "        else:\n",
    "            mode = 'a'\n",
    "        with nc4.Dataset(savename_output, mode) as nc:\n",
    "            ncm = nc.createGroup(model)\n",
    "\n",
    "            # write model description\n",
    "            ncm.setncattr('code', stan_files[model])\n",
    "\n",
    "            if save_stan_output:\n",
    "                if save_only_converged and get_max_rhat(mcmcs) > 1.1:\n",
    "                    logging.warning('Model \"{}\" did not converge -- skipping.'.format(model))\n",
    "                    continue\n",
    "                dimensions = {\n",
    "                    'obstime':int(data['nt_obs']),\n",
    "                    'time':int(data['nt']),\n",
    "                    'sizeclass':int(data['m']),\n",
    "                    'm_minus_j_plus_1':int(data['m']-data['delta_v_inv']),\n",
    "                    'm_minus_1':int(data['m']-1),\n",
    "                    'knots_minus_1':int(6-1),  # hardcoded, adjust for varying nknots\n",
    "                    'sample': mcmcs['mod_obspos'].shape[0],\n",
    "                }\n",
    "                dimensions_inv = {v:k for k,v in dimensions.items()}\n",
    "\n",
    "                for d in dimensions:\n",
    "                    if d not in ncm.dimensions:\n",
    "                        ncm.createDimension(d, dimensions[d])\n",
    "\n",
    "                if 'tau[1]' in mcmcs.flatnames:\n",
    "                    dimensions['tau'] = mcmcs['tau'].shape[1]\n",
    "                    dimensions_inv[dimensions['tau']] = 'tau'\n",
    "                    if 'tau' not in ncm.dimensions:\n",
    "                        ncm.createDimension('tau', dimensions['tau'])\n",
    "\n",
    "                if 'time' not in ncm.variables:\n",
    "                    ncm.createVariable('time', int, ('time',))\n",
    "                ncm.variables['time'][:] = int(data['dt']) * np.arange(data['nt'])\n",
    "                ncm.variables['time'].units = 'minutes since start of experiment'\n",
    "\n",
    "                if 'obstime' not in ncm.variables:\n",
    "                    ncm.createVariable('obstime', int, ('obstime',))\n",
    "                ncm.variables['obstime'][:] = data['t_obs'].astype(int)\n",
    "                ncm.variables['obstime'].units = 'minutes since start of experiment'\n",
    "                ncm.variables['obstime'].long_name = 'time of observations'\n",
    "\n",
    "                for v in ('dt', 'm', 'v_min', 'delta_v_inv', 'obs', 'i_test',\n",
    "                          'E', 'obs_count'):\n",
    "                    if isinstance(data[v], int):\n",
    "                        if v not in ncm.variables:\n",
    "                            ncm.createVariable(v, int, zlib=True)\n",
    "                        ncm.variables[v][:] = data[v]\n",
    "                    elif isinstance(data[v], float):\n",
    "                        if v not in ncm.variables:\n",
    "                            ncm.createVariable(v, float, zlib=True)\n",
    "                        ncm.variables[v][:] = data[v]\n",
    "                    else:\n",
    "                        dims = tuple(dimensions_inv[d] for d in data[v].shape)\n",
    "                        if v not in ncm.variables:\n",
    "                            ncm.createVariable(v, data[v].dtype, dims, zlib=True)\n",
    "                        ncm.variables[v][:] = data[v]\n",
    "\n",
    "\n",
    "                varnames = set(v.split('[')[0] for v in mcmcs.flatnames)\n",
    "                if varnames_save is None:\n",
    "                    varnames_curr = varnames\n",
    "                else:\n",
    "                    varnames_curr = varnames_save\n",
    "\n",
    "                for v in varnames_curr:\n",
    "                    if v in varnames:\n",
    "                        dims = tuple(dimensions_inv[d]\n",
    "                                     for d in mcmcs[v].shape)\n",
    "                        if v not in ncm.variables:\n",
    "                            ncm.createVariable(v, float, dims, zlib=True)\n",
    "                        ncm.variables[v][:] = mcmcs[v]\n",
    "                    else:\n",
    "                        logging.warning('Cannot find variable \"{}\" for model \"{}\".'.format(v,\n",
    "                                                                                           model))\n",
    "            else:\n",
    "                if 'sample' not in ncm.dimensions:\n",
    "                    ncm.createDimension('sample',\n",
    "                                        mcmcs['divrate'].shape[0])\n",
    "\n",
    "                if 'divrate' not in ncm.variables:\n",
    "                    ncm.createVariable('divrate', float, ('sample'))\n",
    "\n",
    "                if 'sumsqdiff' not in ncm.variables:\n",
    "                    ncm.createVariable('sumsqdiff', float, ('sample'))\n",
    "\n",
    "                ncm.variables['sumsqdiff'].setncattr('long_name',\n",
    "                                                     'sum of squared column differences')\n",
    "\n",
    "                ncm.variables['divrate'][:] = mcmcs['divrate']\n",
    "\n",
    "                obs = data['obs']\n",
    "\n",
    "                tmp = mcmcs['mod_obspos']\n",
    "                tmp /= np.sum(tmp, axis=1)[:, None, :]\n",
    "                tmp -= obs[None, :, :]\n",
    "                tmp **= 2\n",
    "\n",
    "                if np.all(data['i_test'] == 0):\n",
    "                    ncm.variables['sumsqdiff'][:] = np.mean(np.sum(tmp, axis=1),\n",
    "                                                              axis=1)\n",
    "                    ncm.variables['sumsqdiff'].setncattr('data_used',\n",
    "                                                         'all data')\n",
    "                else:\n",
    "                    ncm.variables['sumsqdiff'][:] = np.mean(np.sum(tmp[:, :, data['i_test'] == 1],\n",
    "                                                                    axis=1), axis=1)\n",
    "                    ncm.variables['sumsqdiff'].setncattr('data_used', 'testing data')\n",
    "\n",
    "                for iv,v in enumerate(('gamma_max', 'rho_max', 'xi',\n",
    "                                       'xir', 'E_star')):\n",
    "                    if v not in ncm.variables:\n",
    "                        ncm.createVariable(v, float, ('model','sample'))\n",
    "                    if v in mcmcs.flatnames:\n",
    "                        ncm.variables[v][:] = mcmcs[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
